{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually Optimize Neural Network Models\n",
    "\n",
    "From Machine Learning Mastery.  \n",
    "\n",
    "https://machinelearningmastery.com/manually-optimize-neural-networks/\n",
    "\n",
    "The code below optimizes a single/multi-layer Perceptron using Hill Climb algorithm.  \n",
    "\n",
    "This was a good practice to code up your own Perceptron and Neural Network - impractical, but still a really good exercise.  Then we apply Hill Climb algorithm to train the network.  We first do this on a single Perceptron, then we run it on Multi Layer perceptron (i.e. Neural Network). \n",
    "\n",
    "I don't see any advantage to Hill Climb over feed forward/back propagation, but Hill Climb is an interesting and fairly trivial way to find local maxima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from math import exp\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = 1000\n",
    "FEATURES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data with features.   Make 1 redundant and 2 useless, so num of features > 3\n",
    "def make_data(data_size, features):\n",
    "    assert(features > 3)\n",
    "    X, y = make_classification(n_samples=data_size, \n",
    "                           n_features=features, \n",
    "                           n_informative=features-3, \n",
    "                           n_redundant=1)\n",
    "    return(np.array(X), np.array(y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize weight and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(DATA_SIZE)\n",
    "b = np.random.rand(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Model\n",
    "Perceptron model takes a row of data and:\n",
    "* Activate: Multiplies weights and add bias, (A = X[i,:] * W + b) \n",
    "* Transfer: Classify the output T = ( A >= 0.0 ? 1 : 0 )\n",
    "\n",
    "Code below uses numpy vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(X, W, b):\n",
    "    activation = np.dot(X, W)\n",
    "    activation = np.add(activation, b)\n",
    "    return(activation)\n",
    "\n",
    "def transfer(activation):\n",
    "    yhat = np.greater_equal(activation, 0.0)\n",
    "    yhat = yhat.astype(int)\n",
    "    return yhat\n",
    "\n",
    "def predict(X, W, b):\n",
    "    yhat = transfer(activate(X, W, b))\n",
    "    return yhat\n",
    "\n",
    "def objective(X, y, W, b):\n",
    "    yhat = predict(X, W, b)\n",
    "    score = accuracy_score(y, yhat)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Perceptron model\n",
    "\n",
    "Make prediction LOOP times and accumulate accuracy score, each time creating new data.  \n",
    "Mean of the accuracy should be about 50% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47986999999999996\n"
     ]
    }
   ],
   "source": [
    "LOOP = 100\n",
    "scores = np.zeros(LOOP)\n",
    "for i in range(1, LOOP):\n",
    "    W = np.random.randn(FEATURES)\n",
    "    X, y = make_data(DATA_SIZE, FEATURES)    \n",
    "    scores[i] = objective(X, y, W, b)\n",
    "\n",
    "score = np.mean(scores)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Hill Climb\n",
    "\n",
    "Stochastic Hill climb tries to find maxima by repeatedly moving in random direction and see if it is higher than current or not.\n",
    "\n",
    "Note: I'm unclear about changing b together with W. i.e. changing slope and offset at the same time.  Should b changed separately with W and see if it improves the score?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill_climb(X, y, objective, W, b, n_iter, step_size):\n",
    "    # print('X shape %s, Y shape %s, W shape %s' % (X.shape, y.shape, W.shape))\n",
    "    score = objective(X, y, W, b)\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        W_next = W + (np.random.randn(FEATURES) * step_size)\n",
    "        b_next = b + (np.random.rand() * step_size)\n",
    "        score_next = objective(X, y, W_next, b_next)\n",
    "        \n",
    "        if score_next > score:\n",
    "            W, b, score = W_next, b_next, score_next\n",
    "            #print('>%d %.5f' % (i, score))\n",
    "    return [W, b, score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Hill Climb to train single perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NITER = 1000\n",
    "STEP_SZ = 0.05\n",
    "TRAIN_TEST_SPLIT = 0.33\n",
    "\n",
    "X, y = make_data(DATA_SIZE, FEATURES)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TRAIN_TEST_SPLIT)\n",
    "\n",
    "W = np.random.randn(X_train.shape[1])\n",
    "b = np.random.rand(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_trained: [ 1.55605377  0.15436247 -0.88840323 -0.12033405  0.21669997] \n",
      "b_trained: [1.52066144] \n",
      "\n",
      "score: 0.631343\n"
     ]
    }
   ],
   "source": [
    "W_trained, b_trained, score_trained = hill_climb(X_train, y_train, objective, W, b, NITER, STEP_SZ)\n",
    "print('W_trained: %s \\nb_trained: %s \\n\\nscore: %f' % (W_trained, b_trained, score_trained))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 62.72727\n"
     ]
    }
   ],
   "source": [
    "yhat = predict(X_test, W_trained, b_trained)\n",
    "score = accuracy_score(y_test, yhat)\n",
    "print('Test Accuracy: %.5f' % (score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron \n",
    "\n",
    "\n",
    "MLP is stacked (layered) perceptrons - Neural Network. \n",
    "\n",
    "When passing output of one layer to the next, we want to pass the binomial probability rather than binary (1 or 0).  For this, we use sigmoid as the transfer function.\n",
    "\n",
    "In addition, there are multiple layers and each layer is made up of multiple nodes.  The nodes must be trained in a nested loop.\n",
    "In reality, each perceptron is represented as weights and offset (W, b), so forward transfer in neural network is just a matter of:\n",
    "\n",
    "* For each layer in network:\n",
    "* Iterate over each node in a layer, calculating (input * W + b) and passing the result as input to next layer\n",
    "\n",
    "I vectorized only the node calculation to follow the shapes of Weights and outputs, but actually the entire layer can be calculated in one shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_transfer(activation):\n",
    "    return 1.0/(1.0+exp(-activation))\n",
    "\n",
    "def mlp_predict(X, network):\n",
    "    transfer_vectorized = np.vectorize(mlp_transfer)\n",
    "    inputs = X\n",
    "    for layer in network:\n",
    "        to_next_layer = None\n",
    "        for W_node, b_node in layer:\n",
    "            output = transfer_vectorized(activate(inputs, W_node, b_node))\n",
    "            to_next_layer = output if to_next_layer is None else np.vstack((to_next_layer, output))\n",
    "        inputs = np.transpose(to_next_layer)\n",
    "    return inputs\n",
    "\n",
    "def mlp_objective(X, y, network):\n",
    "    yhat = mlp_predict(X, network)\n",
    "    yhat = [round(y) for y in yhat]\n",
    "    score = accuracy_score(y, yhat)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test multi layer perceptron (network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MLP score - should be near 0.5:  0.49900\n"
     ]
    }
   ],
   "source": [
    "def test_mlp():\n",
    "    X, y = make_classification(n_samples=1000, n_features=FEATURES, n_informative=2, n_redundant=1, random_state=1)\n",
    "\n",
    "    # construct network\n",
    "    n_hidden = 10\n",
    "    hidden1 = [ (np.random.randn(FEATURES), np.random.rand(1)) for _ in range(n_hidden)]\n",
    "    hidden2 = [ (np.random.randn(n_hidden), np.random.rand(1)) for _ in range(n_hidden)]\n",
    "    output1 = [ (np.random.randn(n_hidden), np.random.rand(1)) ]\n",
    "    network = [hidden1, hidden2, output1]\n",
    "\n",
    "    score = mlp_objective(X, y, network)\n",
    "    print('Test MLP score - should be near 0.5:  %.5f' % (score))\n",
    "    \n",
    "test_mlp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Stochastic Hill Climb to Multi Layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take a step, we randomly change network weights and look at the results.\n",
    "\n",
    "Instead of changing all the weights in the network, we randomly pick a layer, and make changes to the layer nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(network, step_size):\n",
    "    layer_to_change = random.choice(network)\n",
    "    new_nw = list()\n",
    "    for layer in network:\n",
    "        new_layer = list()\n",
    "        for node in layer:\n",
    "            if layer is layer_to_change:\n",
    "                new_W = node[0] + (np.random.randn(node[0].shape[0]) * step_size)\n",
    "                new_b = node[1] + (np.random.randn(node[1].shape[0]) * step_size)\n",
    "                new_node = (new_W, new_b)\n",
    "            else:\n",
    "                new_node = node\n",
    "            new_layer.append(new_node)\n",
    "        new_nw.append(new_layer)\n",
    "    return new_nw\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The hill climb logic simply takes a step by creating a randomly changed network, and see if score improves.  \n",
    "\n",
    "If it does, use the new network and repeat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_hill_climb(X, y, objective, network, n_iter, step_size):\n",
    "    # print('X shape %s, Y shape %s, W shape %s' % (X.shape, y.shape, W.shape))\n",
    "    score = objective(X, y, network)\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        candidate = step(network, step_size)\n",
    "        score_next = objective(X, y, candidate)\n",
    "        \n",
    "        if score_next > score:\n",
    "            network, score = candidate, score_next\n",
    "            print('>%d %f' % (i, score))\n",
    "    return [network, score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP using Hill Climb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0 0.502985\n",
      ">1 0.532836\n",
      ">3 0.619403\n",
      ">5 0.632836\n",
      ">7 0.641791\n",
      ">9 0.655224\n",
      ">11 0.691045\n",
      ">13 0.708955\n",
      ">18 0.735821\n",
      ">22 0.737313\n",
      ">30 0.765672\n",
      ">32 0.800000\n",
      ">37 0.801493\n",
      ">46 0.810448\n",
      ">47 0.828358\n",
      ">53 0.847761\n",
      ">56 0.852239\n",
      ">62 0.855224\n",
      ">69 0.859701\n",
      ">70 0.865672\n",
      ">87 0.870149\n",
      ">113 0.873134\n",
      ">117 0.877612\n",
      ">124 0.880597\n",
      ">129 0.882090\n",
      ">131 0.888060\n",
      ">132 0.897015\n",
      ">221 0.904478\n",
      ">222 0.910448\n",
      ">251 0.916418\n",
      ">339 0.917910\n",
      ">352 0.922388\n",
      ">375 0.928358\n",
      ">417 0.932836\n",
      ">439 0.940299\n",
      ">512 0.941791\n",
      ">535 0.947761\n",
      ">1543 0.949254\n",
      ">1575 0.950746\n",
      ">1735 0.952239\n",
      ">1765 0.953731\n",
      ">1770 0.959701\n",
      ">2114 0.962687\n",
      ">2124 0.964179\n",
      ">2268 0.965672\n",
      ">2311 0.971642\n",
      ">2471 0.973134\n",
      ">2481 0.976119\n",
      ">2570 0.977612\n",
      ">2606 0.979104\n",
      ">2621 0.982090\n",
      "Training score: 0.982090\n"
     ]
    }
   ],
   "source": [
    "X, y = make_data(DATA_SIZE, FEATURES)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "# construct network\n",
    "n_hidden = 20\n",
    "hidden1 = [ (np.random.randn(FEATURES), np.random.rand(1)) for _ in range(n_hidden)]\n",
    "hidden2 = [ (np.random.randn(n_hidden), np.random.rand(1)) for _ in range(n_hidden)]\n",
    "output1 = [ (np.random.randn(n_hidden), np.random.rand(1)) ]\n",
    "network = [hidden1, hidden2, output1]\n",
    "\n",
    "n_iter = 3000\n",
    "step_size = 0.2\n",
    "network, score = mlp_hill_climb(X_train, y_train, mlp_objective, network, n_iter, step_size)\n",
    "print('Training score: %f' % (score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.98209 \n"
     ]
    }
   ],
   "source": [
    "test_score = mlp_objective(X_test, y_test, network)\n",
    "print('Test score: %.5f ' % (score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Hill Climb has trained the network and able to return >85% accuracy on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37-ml",
   "language": "python",
   "name": "py37-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
