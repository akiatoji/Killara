{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Medical Image Analysis with PyTorch \n",
    "\n",
    "## Milestone 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCrop3D:\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 3\n",
    "            self.output_size = output_size\n",
    "    \n",
    "    def __call__(self, samples):\n",
    "        src, target = samples\n",
    "        h, w, l = src.shape\n",
    "        hsize = self.output_size[0]\n",
    "        wsize = self.output_size[1]\n",
    "        lsize = self.output_size[2]\n",
    "        lstart = np.random.randint(0, l-lsize+1)\n",
    "        lend = lstart + lsize\n",
    "        top = np.random.randint(0, h-hsize+1)\n",
    "        bottom = top + hsize\n",
    "        left = np.random.randint(0, w-wsize+1)\n",
    "        right = left + wsize\n",
    "        \n",
    "        s = np.zeros((lsize, hsize, wsize))\n",
    "        t = np.zeros((lsize, hsize, wsize))\n",
    "\n",
    "        s = src[top:bottom, \n",
    "                left:right, \n",
    "                lstart:lend]\n",
    "        t = target[top:bottom, \n",
    "                   left:right,\n",
    "                   lstart:lend]\n",
    "            \n",
    "        s = s[np.newaxis,...]  # add channel axis\n",
    "        t = t[np.newaxis,...]\n",
    "        rv = (torch.from_numpy(s), torch.from_numpy(t))\n",
    "        \n",
    "        return rv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiftiDataset(Dataset):\n",
    "    def __init__(self, source_dir, target_dir, transform=None):\n",
    "        self.source_files = [join(source_dir, f) for f in listdir(source_dir) if isfile(join(source_dir, f))]\n",
    "        self.target_files = [join(target_dir, f) for f in listdir(target_dir) if isfile(join(target_dir, f))]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.source_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src = nib.load(self.source_files[idx]).get_fdata(dtype=np.float32)\n",
    "        target = nib.load(self.target_files[idx]).get_fdata(dtype=np.float32)\n",
    "        sample = (src, target)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(i,o):\n",
    "    return (nn.Conv3d(i,o,3,padding=1,bias=False), \n",
    "            nn.BatchNorm3d(o), \n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "def unet_block(i,m,o):\n",
    "    return nn.Sequential(*conv(i,m),*conv(m,o))\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, s=32):\n",
    "        super().__init__()\n",
    "        self.start = unet_block(1,s,s)\n",
    "        self.down1 = unet_block(s,s*2,s*2)\n",
    "        self.down2 = unet_block(s*2,s*4,s*4)\n",
    "        self.down3 = unet_block(s*4,s*8,s*8)\n",
    "        self.bridge = unet_block(s*8,s*16,s*8)\n",
    "        self.up3 = unet_block(s*16,s*8,s*4)\n",
    "        self.up2 = unet_block(s*8,s*4,s*2)\n",
    "        self.up1 = unet_block(s*4,s*2,s)\n",
    "        self.final = nn.Sequential(*conv(s*2,s),nn.Conv3d(s,1,1))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        r = [self.start(x)]\n",
    "        r.append(self.down1(F.max_pool3d(r[-1],2)))\n",
    "        r.append(self.down2(F.max_pool3d(r[-1],2)))\n",
    "        r.append(self.down3(F.max_pool3d(r[-1],2)))\n",
    "        x = F.interpolate(self.bridge(F.max_pool3d(r[-1],2)),size=r[-1].shape[2:])\n",
    "        x = F.interpolate(self.up3(torch.cat((x,r[-1]),dim=1)),size=r[-2].shape[2:])\n",
    "        x = F.interpolate(self.up2(torch.cat((x,r[-2]),dim=1)),size=r[-3].shape[2:])\n",
    "        x = F.interpolate(self.up1(torch.cat((x,r[-3]),dim=1)),size=r[-4].shape[2:])\n",
    "        x = self.final(torch.cat((x,r[-4]),dim=1))\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet(\n",
      "  (start): Sequential(\n",
      "    (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (down1): Sequential(\n",
      "    (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (down2): Sequential(\n",
      "    (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (down3): Sequential(\n",
      "    (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (4): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (bridge): Sequential(\n",
      "    (0): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (4): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (up3): Sequential(\n",
      "    (0): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (up2): Sequential(\n",
      "    (0): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (up1): Sequential(\n",
      "    (0): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "  )\n",
      "  (final): Sequential(\n",
      "    (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv3d(32, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Unet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.1\n",
    "BATCH_SIZE = 16\n",
    "N_JOBS = 12\n",
    "N_EPOCHS=50\n",
    "\n",
    "tfms = RandomCrop3D((64,64,32))\n",
    "\n",
    "# set up training and validation data loader for nifti images\n",
    "dataset = NiftiDataset('small/t1', 'small/t2', tfms)  # set preload=False if you have limited CPU memory\n",
    "num_train = len(dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(VALIDATION_SPLIT * num_train)\n",
    "valid_idx = np.random.choice(indices, size=split, replace=False)\n",
    "train_idx = list(set(indices) - set(valid_idx))\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_loader = DataLoader(dataset, sampler=train_sampler, batch_size=BATCH_SIZE,\n",
    "                          num_workers=N_JOBS, pin_memory=True)\n",
    "valid_loader = DataLoader(dataset, sampler=valid_sampler, batch_size=BATCH_SIZE,\n",
    "                          num_workers=N_JOBS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = torch.device('cuda:0')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda(device=device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), weight_decay=1e-6)\n",
    "criterion = nn.SmoothL1Loss()  #nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Training Loss: 5.62e-01, Validation Loss: 4.78e-01\n",
      "Epoch: 2 - Training Loss: 4.74e-01, Validation Loss: 1.36e+01\n",
      "Epoch: 3 - Training Loss: 3.35e-01, Validation Loss: 4.60e+01\n",
      "Epoch: 4 - Training Loss: 2.92e-01, Validation Loss: 1.12e+01\n",
      "Epoch: 5 - Training Loss: 2.59e-01, Validation Loss: 8.22e+00\n",
      "Epoch: 6 - Training Loss: 2.12e-01, Validation Loss: 4.39e+00\n",
      "Epoch: 7 - Training Loss: 2.01e-01, Validation Loss: 5.61e-01\n",
      "Epoch: 8 - Training Loss: 1.94e-01, Validation Loss: 2.24e-01\n",
      "Epoch: 9 - Training Loss: 1.85e-01, Validation Loss: 2.40e-01\n",
      "Epoch: 10 - Training Loss: 1.80e-01, Validation Loss: 1.85e-01\n",
      "Epoch: 11 - Training Loss: 1.67e-01, Validation Loss: 1.83e-01\n",
      "Epoch: 12 - Training Loss: 1.65e-01, Validation Loss: 1.59e-01\n",
      "Epoch: 13 - Training Loss: 1.65e-01, Validation Loss: 1.50e-01\n",
      "Epoch: 14 - Training Loss: 1.64e-01, Validation Loss: 1.32e-01\n",
      "Epoch: 15 - Training Loss: 1.72e-01, Validation Loss: 1.31e-01\n",
      "Epoch: 16 - Training Loss: 1.64e-01, Validation Loss: 1.21e-01\n",
      "Epoch: 17 - Training Loss: 1.57e-01, Validation Loss: 1.19e-01\n",
      "Epoch: 18 - Training Loss: 1.59e-01, Validation Loss: 1.06e-01\n",
      "Epoch: 19 - Training Loss: 1.56e-01, Validation Loss: 1.09e-01\n",
      "Epoch: 20 - Training Loss: 1.52e-01, Validation Loss: 1.04e-01\n",
      "Epoch: 21 - Training Loss: 1.51e-01, Validation Loss: 1.02e-01\n",
      "Epoch: 22 - Training Loss: 1.47e-01, Validation Loss: 1.03e-01\n",
      "Epoch: 23 - Training Loss: 1.50e-01, Validation Loss: 1.03e-01\n",
      "Epoch: 24 - Training Loss: 1.49e-01, Validation Loss: 1.13e-01\n",
      "Epoch: 25 - Training Loss: 1.52e-01, Validation Loss: 1.06e-01\n",
      "Epoch: 26 - Training Loss: 1.52e-01, Validation Loss: 1.16e-01\n",
      "Epoch: 27 - Training Loss: 1.40e-01, Validation Loss: 1.03e-01\n",
      "Epoch: 28 - Training Loss: 1.46e-01, Validation Loss: 9.86e-02\n",
      "Epoch: 29 - Training Loss: 1.45e-01, Validation Loss: 9.72e-02\n",
      "Epoch: 30 - Training Loss: 1.40e-01, Validation Loss: 1.19e-01\n",
      "Epoch: 31 - Training Loss: 1.45e-01, Validation Loss: 1.14e-01\n",
      "Epoch: 32 - Training Loss: 1.41e-01, Validation Loss: 1.22e-01\n",
      "Epoch: 33 - Training Loss: 1.39e-01, Validation Loss: 1.13e-01\n",
      "Epoch: 34 - Training Loss: 1.47e-01, Validation Loss: 1.05e-01\n",
      "Epoch: 35 - Training Loss: 1.36e-01, Validation Loss: 1.04e-01\n",
      "Epoch: 36 - Training Loss: 1.41e-01, Validation Loss: 1.04e-01\n",
      "Epoch: 37 - Training Loss: 1.33e-01, Validation Loss: 1.05e-01\n",
      "Epoch: 38 - Training Loss: 1.42e-01, Validation Loss: 1.27e-01\n",
      "Epoch: 39 - Training Loss: 1.41e-01, Validation Loss: 1.42e-01\n",
      "Epoch: 40 - Training Loss: 1.36e-01, Validation Loss: 9.18e-02\n",
      "Epoch: 41 - Training Loss: 1.35e-01, Validation Loss: 1.01e-01\n",
      "Epoch: 42 - Training Loss: 1.39e-01, Validation Loss: 1.08e-01\n",
      "Epoch: 43 - Training Loss: 1.41e-01, Validation Loss: 1.07e-01\n",
      "Epoch: 44 - Training Loss: 1.37e-01, Validation Loss: 9.61e-02\n",
      "Epoch: 45 - Training Loss: 1.40e-01, Validation Loss: 1.11e-01\n",
      "Epoch: 46 - Training Loss: 1.45e-01, Validation Loss: 1.22e-01\n",
      "Epoch: 47 - Training Loss: 1.42e-01, Validation Loss: 1.20e-01\n",
      "Epoch: 48 - Training Loss: 1.41e-01, Validation Loss: 1.21e-01\n",
      "Epoch: 49 - Training Loss: 1.41e-01, Validation Loss: 1.22e-01\n",
      "Epoch: 50 - Training Loss: 1.39e-01, Validation Loss: 1.12e-01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_losses, valid_losses = [], []\n",
    "n_batches = len(train_loader)\n",
    "for t in range(1, N_EPOCHS + 1):\n",
    "    # training\n",
    "    t_losses = []\n",
    "    model.train(True)\n",
    "    for i, (src, tgt) in enumerate(train_loader):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(src)\n",
    "        loss = criterion(out, tgt)\n",
    "        t_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_losses.append(t_losses)\n",
    "\n",
    "    # validation\n",
    "    v_losses = []\n",
    "    model.train(False)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for src, tgt in valid_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            out = model(src)\n",
    "            loss = criterion(out, tgt)\n",
    "            v_losses.append(loss.item())\n",
    "        valid_losses.append(v_losses)\n",
    "\n",
    "    if not np.all(np.isfinite(t_losses)): \n",
    "        raise RuntimeError('NaN or Inf in training loss, cannot recover. Exiting.')\n",
    "    log = f'Epoch: {t} - Training Loss: {np.mean(t_losses):.2e}, Validation Loss: {np.mean(v_losses):.2e}'\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
